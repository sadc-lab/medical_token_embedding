{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45684506-7e92-4618-9272-0c9c1fce3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/venv1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983fc8c4-3baa-4309-8ccf-3897e33cd2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorical(prob, n_samples):\n",
    "    \"\"\"\n",
    "    sample a categorical distribution from a vect of probabilities\n",
    "    \"\"\"\n",
    "    prob = prob.unsqueeze(0).repeat(n_samples, 1)\n",
    "    cum_prob = torch.cumsum(prob, dim=-1)\n",
    "    r = torch.rand(n_samples, 1)\n",
    "    # argmax finds the index of the first True value in the last axis.\n",
    "    samples = torch.argmax((cum_prob > r).int(), dim=-1)\n",
    "    return samples.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f28cf12-8f60-4ddf-8ec2-b804ec0f7caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    if (len(data) > 0) and (data[-1] == ' '):\n",
    "        return data[:-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b295b396-d3e3-47d1-a3c8-20e9e0206c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    #print(word, len(word))\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    if len(word) == 1:\n",
    "        return [word]\n",
    "    \n",
    "    if word[0] in [':', ',', '.']:\n",
    "        return [word[0]] + remove_punctuation(word[1:])\n",
    "    \n",
    "    idx = 0\n",
    "    while (idx < len(word)) and (not word[idx] in ['!', '(', ')', ';']) :\n",
    "        idx += 1\n",
    "    \n",
    "    if idx == 0:\n",
    "        return [word[0]] + remove_punctuation(word[1:])\n",
    "    if idx == len(word):\n",
    "        if word[-1] in ['.', ',', ':']:\n",
    "            return [word[:-1], word[-1]]\n",
    "        else:\n",
    "            return [word]\n",
    "    \n",
    "    return [word[:idx], word[idx]] + remove_punctuation(word[idx+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0fb00a-dd89-4cd1-a9c4-e3fdfdff52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary function for the custom tokenizer\n",
    "def aux(word):\n",
    "    if word.count('.') > 1:\n",
    "        indices = [index for index in range(len(word)) if word[index] == '.']\n",
    "        if indices[1] - indices[0] == 4: #it's a number\n",
    "            return aux(re.sub(\"\\.\", \"\", word))\n",
    "        else: #it's a date or a phone number\n",
    "            pieces = [aux(word[:indices[0]])+'.']\n",
    "            pieces.extend([aux(word[indices[i]+1:indices[i+1]])+'.' for i in range(len(indices)-1)])\n",
    "            pieces.append(aux(word[indices[-1]+1:]))\n",
    "            return \"\".join(pieces)\n",
    "\n",
    "    if word.count(',') > 1:\n",
    "        indices = [index for index in range(len(word)) if word[index] == ',']\n",
    "        if indices[1] - indices[0] == 4: #it's a number\n",
    "            return aux(re.sub(\",\", \"\", word))\n",
    "        else: #it's a list\n",
    "            pieces = [aux(word[:indices[0]])+',']\n",
    "            pieces.extend([aux(word[indices[i]+1:indices[i+1]])+',' for i in range(len(indices)-1)])\n",
    "            pieces.append(aux(word[indices[-1]+1:]))\n",
    "            return \"\".join(pieces)\n",
    "    \n",
    "    search = re.search(r\":\", word)\n",
    "    if search: #it's probably a time (10:30)\n",
    "        return aux(word[:search.start()]) + ':' + aux(word[search.end():])\n",
    "\n",
    "    if ('.' in word) and (',' in word):\n",
    "        print(word)\n",
    "    if ',' in word:\n",
    "        word = re.sub(r',', '.', word)\n",
    "    \n",
    "    return \"{:e}\".format(float(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15bfb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(word):\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec127db-50b6-4e9f-8877-f3dce9dd4c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(word):    \n",
    "    if word == '':\n",
    "        return word    \n",
    "\n",
    "    if re.search(r'o2|Cl2|cl2', word):\n",
    "        #some chemistry notations\n",
    "        return re.sub(r\"2\", \"-2.000000e+00\", word)\n",
    "                \n",
    "    if (word[0] in ['.', ',']):\n",
    "        return word[0] + custom_tokenizer(word[1:])\n",
    "    \n",
    "    if re.search(r\"\\d\", word[0]): #the word start with a digit\n",
    "        idx=0\n",
    "        while (idx<len(word)) and (re.search(r\"(\\d|\\.|,|:)\", word[idx])):\n",
    "            idx += 1 #delimitation of the number\n",
    "        if word[idx-1] in [',', '.', ':']:\n",
    "            return aux(word[:idx-1]) + word[idx-1] + custom_tokenizer(word[idx:])\n",
    "        if (idx == len(word)-1) and (word[idx:] in ['h', 'H', 'q', 'Q']):\n",
    "            #probably time (hours) or medical term (ex 22q)\n",
    "            return aux(word[:idx]) + \"-\" + word[idx]\n",
    "            \n",
    "        if (idx<len(word)) and (re.search(r\"\\A(h|H|q|Q)\\d+\", word[idx:])):\n",
    "            #probably time (hours and minutes) or medical term (ex 22q11)\n",
    "            return aux(word[:idx]) + \"-\" + word[idx] + \"-\" + custom_tokenizer(word[idx+1:])    \n",
    "            \n",
    "        if (idx<len(word)) and (word[idx:] in ['j', 'e', 'er', 'ème', 'eme', 'aire', 'ième', 'ieme', 'ier', 'nd', 'nde', 'ière']):\n",
    "            #probably rank\n",
    "            return aux(word[:idx]) + \"-\" + word[idx:]    \n",
    "        if (idx<len(word)) and (re.search(r\"[a-z]|\\+\", word[idx])):\n",
    "            #probably number + unit (12mm) or operation (ex: 36+2)\n",
    "            return aux(word[:idx]) + \" \" + custom_tokenizer(word[idx:])    \n",
    "        if (idx<len(word)) and (re.search(r\"[A-Z]\", word[idx])):\n",
    "            #probably medical code (2P2)\n",
    "            return aux(word[:idx]) + \"-\" + custom_tokenizer(word[idx:])    \n",
    "        return aux(word[:idx]) + custom_tokenizer(word[idx:])\n",
    "    \n",
    "    search = re.search(r\"[A-Za-z]*m\\d\", word) #looks for m2 or cm3 etc.\n",
    "    if search:\n",
    "        return custom_tokenizer(word[:search.start()]) + word[search.start():search.end()-1] + '-' + aux(word[search.end()-1]) + custom_tokenizer(word[search.end():])\n",
    "\n",
    "    if re.search(r\"\\A\\-[A-Za-z]\", word): #an item withing a list\n",
    "        return \"- \" + custom_tokenizer(word[1:])\n",
    "        \n",
    "    idx = 0\n",
    "    while (idx<len(word)) and (not re.search(r\"(\\d)\", word[idx])):\n",
    "        idx += 1 #delimitation of the not number subsequence\n",
    "    if (idx<len(word)) and (word[idx-1] in ['-', '/', '%']):\n",
    "        #probably a unit (g/2jour), or a negative number or a range of values\n",
    "        return word[:idx] + custom_tokenizer(word[idx:])\n",
    "\n",
    "    if (idx<len(word)) and (re.search(r\"[A-Z]\", word[idx-1])):\n",
    "        #probably a medical code G2P2\n",
    "        return word[:idx] + \"-\" + custom_tokenizer(word[idx:])\n",
    "\n",
    "    if idx<len(word):\n",
    "        return word[:idx] + \" \" + custom_tokenizer(word[idx:])\n",
    "    \n",
    "    return word[:idx] + custom_tokenizer(word[idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503cfac0-edb0-49df-b327-5334ac41a069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.233200e+02 mmgH'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"123.32mmgH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b29e2fd-1573-4e00-8a85-6db2f3b8354a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.400000e+01/4.000000e+00/2.002000e+03'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"14/04/2002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2fd6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.000000e+00.4.900000e+01.5.600000e+01.9.800000e+01.3.000000e+01'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"01.49.56.98.30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcb5e7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G-2.000000e+00-P-2.000000e+00'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"G2P2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb91e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.000000e+00-j'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"2j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c058f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.000000e+00-h-3.000000e+01'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"8h30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a852675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.000000e+00-H'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"4H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adea7e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.200000e+01-q-1.100000e+01'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"22q11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8642fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.200000e+01 g/cm/cm-2.000000e+00/m-3.000000e+00'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"32g/cm/cm2/m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce56b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.000000e+01:3.000000e+01:0.000000e+00'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"10:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4928641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.000000e+00-ème'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"2ème\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2669ec6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spo-2.000000e+00'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"Spo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f9c969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.600000e+01 mm-2.000000e+00/m-2.000000e+00'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer('86mm2/m2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "209c9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.600000e+01 + 2.000000e+00'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"36+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8162de92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- G-2.000000e+00-P-2.000000e+00'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"-G2P2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72baa5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.000000e+02-1.150000e+02%'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"100-115%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53816a2e-897c-45ea-9223-f026aa93cb8a",
   "metadata": {},
   "source": [
    "### Tokenize numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84bbcc43-e0b3-438a-9756-a0ce8c69f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "tokenized_test_ds = []\n",
    "tokenized_val_ds = []\n",
    "tokenized_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    tokenized_text = []\n",
    "    new_classes = np.copy(sample['classes'])\n",
    "    cursor = 0\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        tokenized_word = custom_tokenizer(word).split()\n",
    "        if len(tokenized_word) > 1:\n",
    "            new_classes = np.insert(new_classes, cursor+1, [0 for k in range(len(tokenized_word)-1)])\n",
    "            cursor += len(tokenized_word)-1\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print(\"'\", word, \"'\", tokenized_word, \"extracted:\", sample['extracted_from'])\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        cursor += 1\n",
    "\n",
    "    tokenized_sample = {'tokens': sum(tokenized_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_test_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in val_ds:\n",
    "    tokenized_text = []\n",
    "    new_classes = np.copy(sample['classes'])\n",
    "    cursor = 0\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        tokenized_word = custom_tokenizer(word).split()\n",
    "        if len(tokenized_word) > 1:\n",
    "            new_classes = np.insert(new_classes, cursor+1, [0 for k in range(len(tokenized_word)-1)])\n",
    "            cursor += len(tokenized_word)-1\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print(\"'\", word, \"'\", tokenized_word, \"extracted:\", sample['extracted_from'])\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        cursor += 1\n",
    "        \n",
    "    tokenized_sample = {'tokens': sum(tokenized_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_val_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in train_ds:\n",
    "    tokenized_text = []\n",
    "    new_classes = np.copy(sample['classes'])\n",
    "    cursor = 0\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        tokenized_word = custom_tokenizer(word).split()\n",
    "        if len(tokenized_word) > 1:\n",
    "            new_classes = np.insert(new_classes, cursor+1, [0 for k in range(len(tokenized_word)-1)])\n",
    "            cursor += len(tokenized_word)-1\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print(\"'\", word, \"'\", tokenized_word, \"extracted:\", sample['extracted_from'])\n",
    "        #if re.search(r'\\d', word):\n",
    "        #    print(\"'\", word, \"'\", tokenized_word, \"extracted:\", sample['extracted_from'])\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        cursor += 1\n",
    "        \n",
    "    tokenized_sample = {'tokens': sum(tokenized_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_train_ds.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9faad9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized_test_2\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_test_ds, fp)\n",
    " \n",
    "with open(\"tokenized_val_2\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_val_ds, fp)\n",
    " \n",
    "with open(\"tokenized_train_2\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_train_ds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e73bc-8924-40ee-b2cf-dfc7e3cd9c5a",
   "metadata": {},
   "source": [
    "# Creation of the ComNumDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d891f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "len_dataset = 200000\n",
    "candidate_numbers = [k for k in range(len_dataset)]\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee1dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len_dataset//4):\n",
    "    selected_numbers = random.sample(candidate_numbers, k=2)\n",
    "    text = aux(str(selected_numbers[0]/1000)) + ' est supérieur à ' + aux(str(selected_numbers[1]/1000)) + '.'\n",
    "    label = selected_numbers[0] > selected_numbers[1]\n",
    "    dataset.append({'text': text, 'label': label})\n",
    "\n",
    "    for number in selected_numbers:\n",
    "        candidate_numbers.remove(number)\n",
    "\n",
    "    selected_numbers = random.sample(candidate_numbers, k=2)\n",
    "    text = aux(str(selected_numbers[0]/1000)) + ' est inférieur à ' + aux(str(selected_numbers[1]/1000)) + '.'\n",
    "    label = selected_numbers[0] < selected_numbers[1]\n",
    "    dataset.append({'text': text, 'label': label})\n",
    "    for number in selected_numbers:\n",
    "        candidate_numbers.remove(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812e3992-0885-4c53-b9fe-4e5fb0f9e52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911038a9-c64d-4a69-87cc-514eb78a4a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(dataset))\n",
    "mask_arr = (rand < 0.2)\n",
    "mlm_train_ds = []\n",
    "mlm_val_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]:\n",
    "        mlm_val_ds.append(dataset[i])\n",
    "    else:\n",
    "        mlm_train_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feee0eba-f909-440b-8498-e25e1bcc167c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79855 20145\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_train_ds), len(mlm_val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fb0b40-be05-4318-b04c-f65da334ca71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"mlm_val_2\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_val_ds, fp)\n",
    " \n",
    "with open(\"mlm_train_2\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_train_ds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2131b-e8b1-4c16-b0e5-c4b47e0143f4",
   "metadata": {},
   "source": [
    "# Creation of the unlabeled tokenized dataset for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2b960683-fdcf-4ec7-8ffa-14d7fbb8498a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " with open(\"mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "mlm_tokenized_val_ds = []\n",
    "mlm_tokenized_train_ds = []\n",
    "mlm_tokenized_global_ds = []\n",
    "\n",
    "for sample in val_ds:\n",
    "    tokenized_sample = {k:v for (k,v) in sample.items()}\n",
    "    tokenized_sample['tokens'] = sum([custom_tokenizer(word).split() for word in sample['tokens']], [])\n",
    "    mlm_tokenized_val_ds.append(tokenized_sample)\n",
    "    mlm_tokenized_global_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in train_ds:\n",
    "    tokenized_sample = {k:v for (k,v) in sample.items()}\n",
    "    tokenized_sample['tokens'] = sum([custom_tokenizer(word).split() for word in sample['tokens']], [])\n",
    "    mlm_tokenized_train_ds.append(tokenized_sample)\n",
    "    mlm_tokenized_global_ds.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b205ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mlm_tokenized_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_val_ds, fp)\n",
    " \n",
    "with open(\"mlm_tokenized_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_train_ds, fp)\n",
    "\n",
    "with open(\"mlm_tokenized_global\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_global_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2d8748c9-107d-4ed9-89db-4c304a007ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25554"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlm_tokenized_global_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "796a56d1-333f-4445-ab59-edc299db6b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20442 5112\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_tokenized_train_ds), len(mlm_tokenized_val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a820c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
