{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cecf6949-eb7e-4eed-9758-657ae3cdc6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45684506-7e92-4618-9272-0c9c1fce3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/sparknlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983fc8c4-3baa-4309-8ccf-3897e33cd2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorical(prob, n_samples):\n",
    "    \"\"\"\n",
    "    sample a categorical distribution from a vect of probabilities\n",
    "    \"\"\"\n",
    "    prob = prob.unsqueeze(0).repeat(n_samples, 1)\n",
    "    cum_prob = torch.cumsum(prob, dim=-1)\n",
    "    r = torch.rand(n_samples, 1)\n",
    "    # argmax finds the index of the first True value in the last axis.\n",
    "    samples = torch.argmax((cum_prob > r).int(), dim=-1)\n",
    "    return samples.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f28cf12-8f60-4ddf-8ec2-b804ec0f7caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    if (len(data) > 0) and (data[-1] == ' '):\n",
    "        return data[:-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e13a8ea-bce2-4a34-9094-a2bd0a6676a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    #print(word, len(word))\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    if len(word) == 1:\n",
    "        return [word]\n",
    "    \n",
    "    if word[0] in [',', ':', '!', '(', ')', ';', '.']:\n",
    "        if not word[-1] in [',', ':', '!', '(', ')', ';', '.']:\n",
    "            return [word[0]] + remove_punctuation(word[1:])\n",
    "        else:\n",
    "            return [word[0]] + remove_punctuation(word[1:-1]) + [word[-1]]\n",
    "    else:\n",
    "        if not word[-1] in [',', ':', '!', '(', ')', ';', '.']:\n",
    "            return [word]\n",
    "        else:\n",
    "            return remove_punctuation(word[:-1]) + [word[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c170f98-da51-47e5-b979-493a4a8e2083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"NLP_data/2020.06.03_CHUSJ_Data_PatientID.csv\", 'r', encoding = 'ISO-8859-1') as file: #latin-1 delimiter = '\\t'\n",
    "    csvreader = csv.reader((line.replace('\\0','').replace('\\t-', '').replace('\\t', '').replace(' \\x19', \"'\") for line in file))\n",
    "    notes = []\n",
    "    for row in csvreader:\n",
    "        notes.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683fad30-0871-499a-b37a-2e374453f629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"NLP_data/2020.07.06_data_visualization_wnumeric.csv\", \\'r\\', encoding=\\'cp1250\\') as file:\\n    csvreader = csv.reader((line.replace(\\'\\x00\\',\\'\\') for line in file))\\n    count = 0\\n    for row in csvreader:\\n        if count < 7:\\n            print(row)\\n            count += 1\\n        else:\\n            break\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open(\"NLP_data/2020.07.06_data_visualization_wnumeric.csv\", 'r', encoding='cp1250') as file:\n",
    "    csvreader = csv.reader((line.replace('\\0','') for line in file))\n",
    "    count = 0\n",
    "    for row in csvreader:\n",
    "        if count < 7:\n",
    "            print(row)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3315b200-62e9-413d-b9be-1497c7115552",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NLP_data/Labelling Le - 0 to 100.csv\", 'r', encoding = 'utf-8') as file:\n",
    "    csvreader = csv.reader((line.replace('\\0','') for line in file))\n",
    "    labels = []\n",
    "    for row in csvreader:\n",
    "        row[2] = clean_data(row[2])\n",
    "        labels.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dee304f-d7d0-4f89-a268-18cd5f1e0770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 11508)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ed5d3f-568b-4c3c-a322-4f5d8ef0e297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101',\n",
       " '2463011',\n",
       " 'T21 avec CAV complet opéré ce jour Défaillance cardiaque sous lasix   Pancreas annulaire - opéré 21/06/2012: laparotomie et duodenoduodenostomie Reflux gastro-oesophagiené.   2/09/2012: Admis 1 jour pour Intoxication lanoxin.  Résolution rapide avec DigiBind x 1  27/1/2013: Admis 1 jour pour tableau de Gastroentérite  Réhydratation',\n",
       " 'yes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbcb78-7b34-445f-ab39-02931c65d871",
   "metadata": {},
   "source": [
    "# Collecting new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbbdb7e-e660-4b6e-bdf7-9818db194457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '85',\n",
       " 'Valeur Saturation Pulsée en Oxygène',\n",
       " 'Value Saturation Pulsed in Oxygen',\n",
       " 'saturation habituelle',\n",
       " '%']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653e78bc-e247-4adc-a2e0-6febda8ea018",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(100):\n",
    "    texts.append(notes[2*i+2][2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4eb097e-1aed-4298-9edb-060f37474581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    clean_text = []\n",
    "    for word in texts[i]:\n",
    "        cleaned_words = remove_punctuation(word)\n",
    "        clean_text.append(cleaned_words)\n",
    "    texts[i] = sum(clean_text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411d164e-1f26-4f56-949f-11d2b8496d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attr_to_class = {\"Fraction d'éjection\":\"Fraction d'éjection\",\n",
    "                \"Valeur de la fraction d'éjection en Simson\":\"Fraction d'éjection\",\n",
    "                \"Fraction de raccourcissement\": \"Fraction de raccourcissement\",\n",
    "                'Fréquence cardiaque : bradycardie':'Fréquence cardiaque',\n",
    "                'Diamètre Artère Pulmonaire Droite distale':'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Droite proximale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Gauche proximale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Principale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Droite': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Gauche': 'Diamètre Artère Pulmonaire',\n",
    "                'diamètre Artère Pulmonaire': 'Diamètre Artère Pulmonaire',\n",
    "                'diamètre Artère Pulmonaire Droite': 'Diamètre Artère Pulmonaire',\n",
    "                'Saturation pulsée en oxygène': 'Saturation en oxygène',\n",
    "                'Valeur Saturation Pulsée en Oxygène': 'Saturation en oxygène',\n",
    "                'saturation veineuse en oxygène': 'Saturation en oxygène',\n",
    "                'Valeur de la Saturation Pulsée en oxygène': 'Saturation en oxygène',\n",
    "                'saturation artérielle en oxygène': 'Saturation en oxygène',\n",
    "                'Objectif cible de Saturation en Oxygène': 'Saturation en oxygène',\n",
    "                'score apgar à 1 minute': 'apgar',\n",
    "                'score apgar à 10 minutes': 'apgar',\n",
    "                'score apgar à 5 minutes': 'apgar',\n",
    "                \"score d'apgar (à une minute et cinq minutes)\": 'apgar',\n",
    "                \"score d'apgar (à une minute, cinq minutes et 10 minutes)\": 'apgar',\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d4bf11-83f6-4e9f-a683-98721076a089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_index = {\"Fraction d'éjection\": 1,\n",
    "                  \"Fraction de raccourcissement\": 2,\n",
    "                  'Fréquence cardiaque': 3,\n",
    "                  'Diamètre Artère Pulmonaire': 4,\n",
    "                  'Saturation en oxygène': 5,\n",
    "                  'apgar': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c6dc6d-9920-4143-b807-5d072afd7a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fixing some splitted tokens\n",
    "labels[439][1] = '65%'\n",
    "labels[478][1] = '46%'\n",
    "labels[815][1] = '27%'\n",
    "labels[284][1] = '31.3%'\n",
    "labels[310][1] = '40,7%'\n",
    "labels[480][1] = '29%'\n",
    "labels[100][1] = '4,5mm'\n",
    "labels[175][1] = '5.5mm'\n",
    "labels[176][1] = '6.6mm'\n",
    "labels[381][1] = '2.8mm'\n",
    "labels[382][1] = '2.3mm'\n",
    "labels[486][1] = '4.9mm'\n",
    "labels[487][1] = '5.7mm'\n",
    "labels[655][1] = '18mm'\n",
    "labels[4][1] = '80-85%'\n",
    "labels[19][1] = '87%'\n",
    "labels[165][1] = '85-88%'\n",
    "labels[166][1] = '75%'\n",
    "labels[224][1] = '50-65%'\n",
    "labels[228][1] = '70-75%'\n",
    "labels[394][1] = '25%'\n",
    "labels[401][1] = '96%'\n",
    "labels[529][1] = '80-85%'\n",
    "labels[543][1] = '85-90%'\n",
    "labels[550][1] = '65%'\n",
    "labels[601][1] = '65-85%'\n",
    "labels[603][1] = '75%'\n",
    "labels[738][1] = '92%'\n",
    "labels[13][1] = '8-9-9'\n",
    "labels[24][1] = '8-9-9'\n",
    "labels[328][1] = '9-9-10'\n",
    "labels[340][1] = '8-9'\n",
    "labels[396][1] = '8-9'\n",
    "labels[417][1] = '1-2-3'\n",
    "labels[448][1] = '9-9-9-'\n",
    "labels[545][1] = '7-9-10'\n",
    "labels[563][1] = '8-9-9'\n",
    "labels[572][1] = '7-8-10'\n",
    "labels[583][1] = '7-8-8'\n",
    "labels[644][1] = '8-9-9'\n",
    "labels[674][1] = '9-9-9'\n",
    "labels[724][1] = '8-9-9'\n",
    "labels[739][1] = '8/8/9'\n",
    "labels[752][1] = '5.5.7'\n",
    "labels[764][1] = '6/7/9'\n",
    "labels[777][1] = '9/9/9'\n",
    "labels[845][1] = '6/8/8'\n",
    "labels[913][1] = '9-9-9'\n",
    "labels[6][1] = '6,5'\n",
    "labels[7][1] = '7,1'\n",
    "labels[538][1] = '1,6'\n",
    "labels[886][1] = '60-68'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5011f21-bd80-4257-b5b7-b28b5489cf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_to_ignore = [5, 26, 116, 121, 131, 144, 160, 341, 356, 357, 379, 397, 405, 406, 418, 419, 449, 450, 522, 530, 537, 544, 546, 547, 564, 565, 573, 574, 584, 585, 605, 606, 607, 608, 645, 646, 648, 675, 676, 704, 725, 726, 740, 741, 753, 754, 765, 766, 778, 779, 795, 801, 846, 847, 887, 914, 915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "560f1a4c-f9ba-4ddc-8cb2-41fe72a0c95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = {k:[] for k in range(100)}\n",
    "for i in range(len(labels)):\n",
    "    if labels[i][2] in attr_to_class:\n",
    "        classe = attr_to_class[labels[i][2]]\n",
    "        if not (i in lines_to_ignore):\n",
    "            patient_id = int(labels[i][0])\n",
    "            classes[patient_id].append((labels[i][1], classe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50c47cb-3320-4762-a818-aa4f5274b360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of doublons\n",
      "43 value= 7 class= 6 pos= [23, 25]\n",
      "43 value= 7 class= 6 pos= [23, 25]\n",
      "52 value= 24 class= 1 pos= [143, 224]\n",
      "52 value= 23 class= 2 pos= [135, 216]\n",
      "69 value= 11 class= 4 pos= [136, 182]\n",
      "69 value= 11 class= 4 pos= [136, 182]\n",
      "69 value= 10 class= 4 pos= [144, 187]\n",
      "70 value= 5.5 class= 4 pos= [100, 106]\n"
     ]
    }
   ],
   "source": [
    "pos_classes = {k:np.zeros(len(texts[k]), dtype=int) for k in range(100)}\n",
    "print(\"list of doublons\")\n",
    "for i in range(100):\n",
    "    for (value, classe) in classes[i]:\n",
    "        if not value == '9 -9-10': #special case to deal with later\n",
    "            index_class = class_to_index[classe]\n",
    "            index = texts[i].index(value)\n",
    "            indices = [k for k in range(len(texts[i])) if texts[i][k]==value]\n",
    "            if len(indices)> 1:\n",
    "                print(i, 'value=', value, 'class=', index_class, 'pos=', indices)\n",
    "            pos_classes[i][index] = index_class\n",
    "        else:\n",
    "            index_class = class_to_index[classe]\n",
    "            index = texts[i].index('-9-10')\n",
    "            pos_classes[i][index] = index_class\n",
    "            pos_classes[i][index-1] = index_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "532a5296-40d2-409b-a2bf-bd5a1364915f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dealing with numerical values doublons in single notes\n",
    "pos_classes[43][23] = 6\n",
    "pos_classes[43][25] = 6\n",
    "pos_classes[52][143] = 1\n",
    "pos_classes[52][224] = 1\n",
    "pos_classes[52][135] = 2\n",
    "pos_classes[52][216] = 2\n",
    "pos_classes[69][136] = 4\n",
    "pos_classes[69][182] = 4\n",
    "pos_classes[69][144] = 4\n",
    "pos_classes[70][100] = 0\n",
    "pos_classes[70][106] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f0855d4-f2c3-4b28-b1af-0295146ab385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_classes[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1694428d-d03f-49be-b5bd-6288cd119c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_classes[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "197ab908-83a5-4ccf-971f-d588c9ffd6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8993191-b926-4ec8-b9d0-046864e07b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "minimal_sentence_size = 7\n",
    "for i in range(100):\n",
    "    text = texts[i]\n",
    "    class_indexes = pos_classes[i]\n",
    "    #listing the breaking points while avoiding breaks like \"Dr. Fournier.\"\n",
    "    breaks = [-1]\n",
    "    for j in range(len(text)):\n",
    "        if (text[j] == '.') and (j > breaks[-1] + minimal_sentence_size):\n",
    "            breaks.append(j)\n",
    "    if breaks[-1]!= len(text)-1:\n",
    "        breaks.append(len(text)-1)\n",
    "        \n",
    "    for j in range(len(breaks)-1):\n",
    "        sample = {'tokens': text[breaks[j]+1: breaks[j+1]+1], \n",
    "                  'classes': class_indexes[breaks[j]+1: breaks[j+1]+1],\n",
    "                  'extracted_from': i}\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3386f6-561f-4ca5-94fc-c917982ff80d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a161a1-14eb-4065-aeb1-1de99a669fe3",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7dd1416-aae3-4671-9fda-f4b4c7b76526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_sample = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[]}\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    for class_idx in sample['classes']:\n",
    "        if class_idx != 0:\n",
    "            class_to_sample[class_idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bf80d7b-c881-44ec-860f-3aaaafc3336d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([232, 249, 250, 254, 413]),\n",
       " 2: array([163, 177, 249, 249, 253]),\n",
       " 3: array([432, 433]),\n",
       " 4: array([ 12,  13,  55, 104, 105, 211, 211, 261, 261, 286, 288, 330, 330,\n",
       "        333, 333, 342, 424, 424]),\n",
       " 5: array([  7,   8,  90,  91, 129, 207, 218, 270, 278, 305, 321, 321, 373]),\n",
       " 6: array([  8,  21,  46,  49,  60,  67,  87,  98, 129, 143, 143, 158, 179,\n",
       "        186, 205, 205, 205, 214, 216, 216, 216, 222, 241, 278, 307, 311,\n",
       "        316, 322, 338, 359, 372, 375, 385, 400, 425, 450])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_sample= {k:np.array(v) for k,v in class_to_sample.items()}\n",
    "class_to_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "678094ba-501e-4e4e-924e-f31b0a0fd4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_arr = categorical(torch.tensor([.15, .15, .7]), len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "500d2cad-0aee-4b37-b19a-7fa35fd839a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 1, 0, 2, 0, 2, 0, 2, 2, 2,\n",
       "       1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 1, 2, 0, 2,\n",
       "       2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0,\n",
       "       2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0,\n",
       "       2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 2, 2, 1, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 2, 0, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 2, 0, 0, 0, 1, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0,\n",
       "       2, 0, 2, 2, 1, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2,\n",
       "       2, 1, 2, 1, 2, 1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 0, 2, 2, 1, 2, 0,\n",
       "       0, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 1,\n",
       "       2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 0, 0, 2, 2,\n",
       "       2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 0, 0, 2, 2, 1, 2, 0, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1,\n",
       "       0, 0, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 1, 2, 1, 1, 2, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2052dda-6f53-4892-86ae-1a44c2987d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr[class_to_sample[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42d0d484-6d18-451a-8bc5-6f97238da831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#balancing the datasets (test, val, train)\n",
    "mask_arr[232] = 2\n",
    "mask_arr[253] = 1\n",
    "mask_arr[163] = 2\n",
    "mask_arr[261] = 0\n",
    "mask_arr[424] = 0\n",
    "mask_arr[91] = 1\n",
    "mask_arr[129] = 1\n",
    "mask_arr[87] = 0\n",
    "mask_arr[98] = 0\n",
    "mask_arr[359] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b142e33-8772-4e2b-b000-70bc4f0ffc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]==0:\n",
    "        test_ds.append(dataset[i])\n",
    "    elif mask_arr[i]==1:\n",
    "        val_ds.append(dataset[i])\n",
    "    else:\n",
    "        train_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9efddb6-8362-471b-b062-22dd3de5b8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 64 81\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "274b290a-e838-489d-8c5d-1c471f45d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(test_ds, fp)\n",
    " \n",
    "#with open(\"test\", \"rb\") as fp:   # Unpickling\n",
    "#   test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(val_ds, fp)\n",
    " \n",
    "#with open(\"val\", \"rb\") as fp:   # Unpickling\n",
    "#   val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(train_ds, fp)\n",
    " \n",
    "#with open(\"train\", \"rb\") as fp:   # Unpickling\n",
    "#   train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59615c4-202a-4727-9104-e008dc339edb",
   "metadata": {},
   "source": [
    "# Blind dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "396f9395-de7b-48f7-aecb-1081230b430e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('camembert-bio-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4aff1152-f4ef-4933-ae9c-6ce72b44801a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "\n",
    "blind_test_ds = []\n",
    "blind_val_ds = []\n",
    "blind_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    blind_sample = {k:v for (k,v) in sample.items()}\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if sample['classes'][i] != 0:\n",
    "            blind_sample['tokens'][i] = 'nombre'\n",
    "    blind_test_ds.append(blind_sample)\n",
    "            \n",
    "for sample in val_ds:\n",
    "    blind_sample = {k:v for (k,v) in sample.items()}\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if sample['classes'][i] != 0:\n",
    "            blind_sample['tokens'][i] = 'nombre'\n",
    "    blind_val_ds.append(blind_sample)\n",
    "    \n",
    "for sample in train_ds:\n",
    "    blind_sample = {k:v for (k,v) in sample.items()}\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if sample['classes'][i] != 0:\n",
    "            blind_sample['tokens'][i] = 'nombre' \n",
    "    blind_train_ds.append(blind_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5add76c4-0c1d-4b6a-8c05-a0431475b7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"blind_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_test_ds, fp)\n",
    " \n",
    "#with open(\"blind_test\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"blind_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_val_ds, fp)\n",
    " \n",
    "#with open(\"blind_val\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"blind_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_train_ds, fp)\n",
    " \n",
    "#with open(\"blind_train\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b5409c8-b92a-46c9-b52b-f286aa3e72f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 64 81\n"
     ]
    }
   ],
   "source": [
    "print(len(blind_train_ds), len(blind_val_ds), len(blind_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4726825-11a3-45ce-9286-d57243f438f3",
   "metadata": {},
   "source": [
    "# FOR LATER, OTHER ATTRIBUTES TO ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca08fa-0877-490a-8eff-087f9a1b95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "attr_to_class = {'2ème geste et 2ème pare': 'geste et pare',\n",
    "                '3ème geste et 3ème pare': 'geste et pare',\n",
    "                'Abbréviation I pour 1': 'chiffre romain',\n",
    "                'Abbréviation deux dimensions': 'dimensions',\n",
    "                'Age (heures)': 'age',\n",
    "                'Age (semaines)': 'age',\n",
    "                'Age (jours)': 'age',\n",
    "                'Date (année)': 'date',\n",
    "                }\n",
    "            \n",
    "#fixing some splitted tokens\n",
    "labels[213][1] = 'J1'\n",
    "labels[214][1] = 'J2'\n",
    "labels[407][1] = 'J7'\n",
    "labels[421][1] = '8j'\n",
    "labels[705][1] = 'J6'\n",
    "labels[732][1] = 'J2'\n",
    "labels[771][1] = 'J4'\n",
    "labels[521][1] = '07/2014'\n",
    "labels[522][1] = '2-3'\n",
    "labels[531][1] = '2013-04-03'\n",
    "labels[532][1] = '2013-03-29'\n",
    "labels[new][1] = '2013-05-15'\n",
    "labels[540][2] = 'Age (semaines)'\n",
    "\n",
    "# need space before 2 à 4mois in notes[148] (should not alter note though)\n",
    "# pay attention to parenthesis when extracting data\n",
    "# add 2-3 mois as a 'duration' to note[118]\n",
    "# pay attention to -2010, ... in notes[106]\n",
    "# missing label for the third date in notes[122]\n",
    "# remove space in 9 -9-10 to get 9-9-10 in notes[58]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e73bc-8924-40ee-b2cf-dfc7e3cd9c5a",
   "metadata": {},
   "source": [
    "# Creation of the unlabeled dataset for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a14e793-3f2d-4008-99a5-e802baba7811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(5753):\n",
    "    texts.append(notes[2*i+2][2].split())\n",
    "\n",
    "for i in range(5753):\n",
    "    clean_text = []\n",
    "    for word in texts[i]:\n",
    "        cleaned_words = remove_punctuation(word)\n",
    "        clean_text.append(cleaned_words)\n",
    "    texts[i] = sum(clean_text, [])\n",
    "    \n",
    "dataset = []\n",
    "minimal_sentence_size = 7\n",
    "for i in range(5753):\n",
    "    text = texts[i]\n",
    "    #listing the breaking points while avoiding breaks like \"Dr. Fournier.\"\n",
    "    breaks = [-1]\n",
    "    for j in range(len(text)):\n",
    "        if (text[j] == '.') and (j > breaks[-1] + minimal_sentence_size):\n",
    "            breaks.append(j)\n",
    "    if breaks[-1]!= len(text)-1:\n",
    "        breaks.append(len(text)-1)\n",
    "        \n",
    "    for j in range(len(breaks)-1):\n",
    "        sample = {'tokens': text[breaks[j]+1: breaks[j+1]+1], \n",
    "                  'extracted_from': i}\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "812e3992-0885-4c53-b9fe-4e5fb0f9e52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26166"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911038a9-c64d-4a69-87cc-514eb78a4a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(dataset))\n",
    "mask_arr = (rand < 0.15)\n",
    "mlm_train_val_ds = []\n",
    "mlm_test_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]:\n",
    "        mlm_test_ds.append(dataset[i])\n",
    "    else:\n",
    "        mlm_train_val_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716f45a1-38fc-4f2d-b44c-d4d34781cff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(mlm_train_val_ds))\n",
    "mask_arr = (rand < 0.15)\n",
    "mlm_train_ds = []\n",
    "mlm_val_ds = []\n",
    "for i in range(len(mlm_train_val_ds)):\n",
    "    if mask_arr[i]:\n",
    "        mlm_val_ds.append(mlm_train_val_ds[i])\n",
    "    else:\n",
    "        mlm_train_ds.append(mlm_train_val_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feee0eba-f909-440b-8498-e25e1bcc167c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18868 3339 3959\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_train_ds), len(mlm_val_ds), len(mlm_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fb0b40-be05-4318-b04c-f65da334ca71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"mlm_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_test_ds, fp)\n",
    " \n",
    "#with open(\"mlm_test\", \"rb\") as fp:   # Unpickling\n",
    "#   mlm_test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"mlm_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_val_ds, fp)\n",
    " \n",
    "#with open(\"mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "#   mlm_val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"mlm_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_train_ds, fp)\n",
    " \n",
    "#with open(\"mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "#   mlm_train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94772a-f6b3-423e-8463-962922c84542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "sparknlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
